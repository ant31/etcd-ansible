# Production etcd cluster inventory with Smallstep CA
#
# Copy this file to inventory.ini and customize for your environment.
# Then configure secrets in group_vars/all/vault.yml (see vault.yml.example)
#
# Usage:
#   Create:     make create-cluster    (or: ansible-playbook -i inventory.ini playbooks/etcd-cluster.yaml -e etcd_action=create -b)
#   Health:     make health-check      (or: ansible-playbook -i inventory.ini playbooks/etcd-health.yaml)
#   Upgrade:    make upgrade-cluster   (or: ansible-playbook -i inventory.ini playbooks/upgrade-cluster.yaml -e etcd_version=v3.5.26 -b)
#   Backup:     make backup-cluster    (or: ansible-playbook -i inventory.ini playbooks/etcd-cluster.yaml -e etcd_action=backup -b)
#   Delete:     make delete-cluster    (or: ansible-playbook -i inventory.ini playbooks/etcd-cluster.yaml -e etcd_delete_cluster=true -b)
#
# Network requirements:
#   - Etcd nodes must reach each other on ports 2379 (client) and 2380 (peer)
#   - All nodes must reach cert-manager on port 9000 (step-ca)
#   - Firewall: Allow TCP 2379, 2380, 9000 between etcd nodes

# ===========================
# ETCD CLUSTER NODES
# ===========================
# Use odd number (3, 5, 7) for proper quorum
# For production: minimum 3 nodes (tolerates 1 failure)
# For high availability: 5 nodes (tolerates 2 failures)

[etcd]
etcd-k8s-1 ansible_host=10.0.1.10 ip=10.0.1.10
etcd-k8s-2 ansible_host=10.0.1.11 ip=10.0.1.11
etcd-k8s-3 ansible_host=10.0.1.12 ip=10.0.1.12

# ===========================
# CLIENT CERTIFICATE NODES
# ===========================
# Nodes that need client certificates to connect to etcd
# Examples: Kubernetes API servers, monitoring systems, backup tools

[etcd_clients]
kube-apiserver-1 ansible_host=10.0.2.10 ip=10.0.2.10
kube-apiserver-2 ansible_host=10.0.2.11 ip=10.0.2.11
kube-apiserver-3 ansible_host=10.0.2.12 ip=10.0.2.12

# ===========================
# CERTIFICATE AUTHORITY NODES
# ===========================
# Nodes that run step-ca and hold CA keys
#
# DEVELOPMENT (single cert-manager):
#   [etcd_cert_managers]
#   etcd-k8s-1
#
# PRODUCTION (HA with backup):
#   [etcd_cert_managers]
#   etcd-k8s-1  # Primary - step-ca RUNNING
#   etcd-k8s-2  # Backup - CA keys replicated, step-ca STOPPED (ready for failover)

[etcd_cert_managers]
etcd-k8s-1  # Primary cert-manager (step-ca active)
etcd-k8s-2  # Backup cert-manager (step-ca installed but stopped)

# ===========================
# COMBINED GROUPS
# ===========================
# Do not modify these

[etcd_all:children]
etcd
etcd_clients

# ===========================
# GLOBAL VARIABLES
# ===========================

[all:vars]
# SSH connection settings
ansible_user=ubuntu
ansible_python_interpreter=/usr/bin/python3
# ansible_ssh_private_key_file=~/.ssh/id_rsa

# Cluster identity
etcd_cluster_name=k8s

# Etcd version
etcd_version=v3.5.26

# Ports (defaults - usually don't need to change)
# etcd_ports:
#   client: 2379
#   peer: 2380

# Paths (defaults - usually don't need to change)
# bin_dir=/opt/bin
# etcd_home=/var/lib/etcd
# etcd_config_dir=/etc/etcd
# etcd_cert_dir=/etc/etcd/ssl

# ===========================
# BACKUP CONFIGURATION
# ===========================
# Configure these in group_vars/all/vault.yml (encrypted with ansible-vault)
#
# Required variables:
#   step_ca_password: "your-secure-ca-password"
#   step_provisioner_password: "your-secure-provisioner-password"
#   step_ca_backup_s3_bucket: "your-org-etcd-backups"
#   etcd_upload_backup:
#     storage: s3
#     bucket: "your-org-etcd-backups"
#   step_ca_backup_kms_key_id: "alias/etcd-ca-backup"
#
# Optional: AWS credentials (if not using IAM roles)
#   aws_access_key_id: "AKIAIOSFODNN7EXAMPLE"
#   aws_secret_access_key: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
#   aws_default_region: "us-east-1"

# Backup scheduling (enabled by default)
etcd_backup_cron_enabled=true
ca_backup_cron_enabled=true

# Optional: Deadman monitoring (get alerts if backups stop working)
# backup_healthcheck_enabled=true
# backup_healthcheck_url=https://hc-ping.com/your-uuid
# ca_backup_healthcheck_url=https://hc-ping.com/your-ca-uuid

# ===========================
# NEXT STEPS
# ===========================
# 1. Copy this file:
#      cp inventory-example.ini inventory.ini
#
# 2. Edit inventory.ini with your actual nodes
#
# 3. Create and configure vault.yml:
#      cp group_vars/all/vault.yml.example group_vars/all/vault.yml
#      vi group_vars/all/vault.yml  # Add your secrets
#      ansible-vault encrypt group_vars/all/vault.yml
#      echo "your-vault-password" > ~/.vault-pass && chmod 600 ~/.vault-pass
#
# 4. Setup AWS KMS (if using aws-kms encryption):
#      make setup-kms
#
# 5. Deploy cluster:
#      make create-cluster
#
# 6. Verify:
#      make health-check
