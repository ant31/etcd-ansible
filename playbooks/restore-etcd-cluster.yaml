---
# Restore etcd cluster from backup (Disaster Recovery)
# Usage: ansible-playbook -i inventory.ini playbooks/restore-etcd-cluster.yaml
#
# This follows the etcd disaster recovery procedure:
# 1. Stop ALL etcd services
# 2. Restore snapshot on ALL nodes
# 3. Start ALL etcd services together
#
# Options:
#   -e restore_ca=true                    # Also restore CA
#   -e restore_etcd_s3_file=path/to/file  # Specific backup file
#   -e restore_confirm=false              # Skip confirmation prompt
#   -e restore_bump_revision=1000000000   # Revision bump (default: 1 billion)

- name: Confirm cluster restore
  hosts: localhost
  gather_facts: no
  tasks:
    - name: Warning about cluster restore
      pause:
        prompt: |
          ⚠️  DISASTER RECOVERY: ETCD CLUSTER RESTORE
          
          This will:
          1. STOP all etcd services
          2. REPLACE cluster data with backup
          3. RESTART cluster with new data
          
          Cluster: {{ groups['etcd'] | length }} nodes
          
          ⚠️  ALL NODES WILL BE UNAVAILABLE DURING RESTORE
          
          Continue? (yes/no)
      register: restore_confirm_prompt
      when: restore_confirm | default(true) | bool
    
    - name: Validate confirmation
      assert:
        that:
          - not (restore_confirm | default(true) | bool) or restore_confirm_prompt.user_input == 'yes'
        fail_msg: "❌ Cluster restore cancelled"
      when: restore_confirm | default(true) | bool

- name: Stop all etcd services
  hosts: etcd
  gather_facts: yes
  roles:
    - etcd3/facts
  tasks:
    - name: Stop etcd service on all nodes
      systemd:
        name: "{{ etcd_name }}"
        state: stopped
      failed_when: false
      tags:
        - restore-stop

- name: Restore snapshot on all nodes
  hosts: etcd
  gather_facts: yes
  roles:
    - role: etcd3/restore
      vars:
        restore_etcd_data: true
        restore_ca: "{{ restore_ca_also | default(false) | bool }}"
        restore_confirm: false  # Already confirmed above
  tags:
    - restore-data

- name: Start all etcd services
  hosts: etcd
  gather_facts: no
  roles:
    - etcd3/facts
  tasks:
    - name: Start etcd service
      systemd:
        name: "{{ etcd_name }}"
        state: started
        enabled: yes
      register: etcd_start
      tags:
        - restore-start
    
    - name: Wait for etcd to be ready
      command: >
        {{ bin_dir }}/etcdctl
        --endpoints={{ etcd_client_url }}
        --cert={{ etcd_cert_paths.client.cert }}
        --cacert={{ etcd_cert_paths.client.ca }}
        --key={{ etcd_cert_paths.client.key }}
        endpoint health
      register: etcd_health
      until: etcd_health.rc == 0
      retries: 20
      delay: 5
      environment:
        ETCDCTL_API: "3"
      changed_when: false
      tags:
        - restore-start

- name: Verify cluster health
  hosts: etcd[0]
  gather_facts: no
  roles:
    - etcd3/facts
  tasks:
    - name: Check cluster status
      command: >
        {{ bin_dir }}/etcdctl --endpoints={{ etcd_access_addresses }}
        --cert={{ etcd_cert_paths.client.cert }}
        --cacert={{ etcd_cert_paths.client.ca }}
        --key={{ etcd_cert_paths.client.key }}
        endpoint health
      environment:
        ETCDCTL_API: "3"
      register: cluster_health
      changed_when: false
      tags:
        - restore-verify
    
    - name: Display restore result
      debug:
        msg: |
          ✅ CLUSTER RESTORE COMPLETE
          
          {{ cluster_health.stdout }}
          
          All nodes restored from backup and restarted.
          
          Important notes:
          - Revision was bumped by {{ restore_bump_revision | default(1000000000) }}
          - All watches were invalidated (mark-compacted)
          - Old revisions are not accessible
          
          If using Kubernetes, restart controllers to refresh caches.
      tags:
        - restore-verify
